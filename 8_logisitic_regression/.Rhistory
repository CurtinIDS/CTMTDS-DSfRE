electricity = pd.read_csv("7_time_series/data/energy_dataset_Yt.csv")
#| echo: False
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
import pandas as pd
from matplotlib.patches import Rectangle
import os
electricity = pd.read_csv("7_time_series/data/energy_dataset_Yt.csv")
electricity = pd.read_csv("data/energy_dataset_Yt.csv")
electricity = pd.read_csv("data/energy_dataset_Yt.csv")
electricity
electricity = pd.read_csv("data/energy_dataset_Yt.csv")
electricity["timestamp"] = pd.to_datetime(electricity["timestamp"])
#| echo: False
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from utilsforecast.plotting import plot_series as plot_series_utils
from scipy.stats import pearsonr
import statsmodels.api as sm
from statsmodels.graphics.tsaplots import plot_acf
plt.style.use("ggplot")
import matplotlib as mpl
from cycler import cycler
mpl.rcParams['axes.prop_cycle'] = cycler(color=["#000000", "#000000"])
plt.rcParams.update({
"figure.figsize": (8, 5),
"figure.dpi": 100,
"savefig.dpi": 300,
"figure.constrained_layout.use": True,
"axes.titlesize": 12,
"axes.labelsize": 10,
"xtick.labelsize": 9,
"ytick.labelsize": 9,
"legend.fontsize": 9,
"legend.title_fontsize": 10,
})
from fpppy.utils import plot_series
pd.set_option("max_colwidth", 100)
pip3 install StatsForecast
#| echo: False
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from utilsforecast.plotting import plot_series as plot_series_utils
from scipy.stats import pearsonr
import statsmodels.api as sm
from statsmodels.graphics.tsaplots import plot_acf
plt.style.use("ggplot")
import matplotlib as mpl
from cycler import cycler
mpl.rcParams['axes.prop_cycle'] = cycler(color=["#000000", "#000000"])
plt.rcParams.update({
"figure.figsize": (8, 5),
"figure.dpi": 100,
"savefig.dpi": 300,
"figure.constrained_layout.use": True,
"axes.titlesize": 12,
"axes.labelsize": 10,
"xtick.labelsize": 9,
"ytick.labelsize": 9,
"legend.fontsize": 9,
"legend.title_fontsize": 10,
})
from fpppy.utils import plot_series
pd.set_option("max_colwidth", 100)
ansett = pd.read_csv("data/ansett.csv")
ansett["ds"] = pd.to_datetime(ansett["ds"])
melsyd_economy = ansett.query(
'Airports == "MEL-SYD" & Class == "Economy"'
).copy()
melsyd_economy["y"] = melsyd_economy["y"] / 1000
plot_series(df=melsyd_economy,
id_col="Airports",
time_col="ds",
target_col="y",
ylabel="Passengers ('000)",
xlabel="Week [1W]",
title="Ansett airlines economy class: Melbourne-Sydney"
)
from utilsforecast.plotting import plot_series as plot_series_utils
import numpy as np
import pandas as pd
import seaborn as sns
import seaborn as sns
import matplotlib.pyplot as plt
from utilsforecast.plotting import plot_series as plot_series_utils
from scipy.stats import pearsonr
import statsmodels.api as sm
from statsmodels.graphics.tsaplots import plot_acf
df = pd.read_csv("data/ansett.csv")
# Convert 'ds' column to datetime
df['ds'] = pd.to_datetime(df['ds'])
# Filter for Melbourne-Sydney route and Economy class
mel_syd_economy = df[(df['Airports'] == 'MEL-SYD') & (df['Class'] == 'Economy')]
# Plot using seaborn
plt.figure(figsize=(14, 6))
sns.lineplot(data=mel_syd_economy, x='ds', y='y')
plt.title('Passenger Counts for Melbourne-Sydney (Economy Class)')
plt.xlabel('Date')
plt.ylabel('Passenger Count')
plt.tight_layout()
plt.grid(True)
plt.show()
df = pd.read_csv("data/ansett.csv")
# Convert 'ds' column to datetime
df['ds'] = pd.to_datetime(df['ds'])
# Filter for Melbourne-Sydney route and Economy class
mel_syd_economy = df[(df['Airports'] == 'MEL-SYD') & (df['Class'] == 'Economy')]
# Plot using seaborn
# Create the plot using constrained_layout instead of tight_layout
fig, ax = plt.subplots(figsize=(14, 6), constrained_layout=True)
sns.lineplot(data=mel_syd_economy, x='ds', y='y', ax=ax)
ax.set_title('Passenger Counts for Melbourne-Sydney (Economy Class)')
ax.set_xlabel('Date')
ax.set_ylabel('Passenger Count')
ax.grid(True)
plt.show()
#| echo: False
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
df = pd.read_csv("data/ansett.csv")
# Convert 'ds' column to datetime
df['ds'] = pd.to_datetime(df['ds'])
# Filter for Melbourne-Sydney route and Economy class
mel_syd_economy = df[(df['Airports'] == 'MEL-SYD') & (df['Class'] == 'Economy')]
# Plot using seaborn
# Create the plot using constrained_layout instead of tight_layout
fig, ax = plt.subplots(figsize=(14, 5.5), constrained_layout=True)
sns.lineplot(data=mel_syd_economy, x='ds', y='y', ax=ax)
# ax.set_title('Passenger Counts for Melbourne-Sydney (Economy Class)')
ax.set_xlabel('Date')
ax.set_ylabel('Passenger Count')
ax.grid(True)
plt.show()
# Load the dataset
beer_df = pd.read_csv("data/aus_production.csv")
# Convert 'ds' to datetime format
beer_df['ds'] = pd.to_datetime(beer_df['ds'])
# Plot the Beer production time series
plt.figure(figsize=(14, 5.5))
sns.lineplot(data=beer_df, x='ds', y='Beer')
# plt.title('Beer Production in Australia (1956–Present)')
plt.xlabel('Date')
plt.ylabel('Beer Production')
plt.grid(True)
plt.tight_layout()
plt.show()
# Create necessary time-related columns
beer_df['Year'] = beer_df['ds'].dt.year
beer_df['Quarter'] = beer_df['ds'].dt.quarter
beer_df['Quarter_Label'] = beer_df['Quarter'].map({1: 'Q1', 2: 'Q2', 3: 'Q3', 4: 'Q4'})
# Plot beer production over time by quarter
plt.figure(figsize=(16, 5.5))
sns.lineplot(data=beer_df, x='Year', y='Beer', hue='Quarter_Label', marker='o')
plt.xlabel('Year')
plt.ylabel('Beer Production')
plt.grid(True)
plt.legend(title='Quarter')
plt.tight_layout()
plt.show()
# Load and parse the SOI text file
with open("data/soi.txt", "r") as file:  # Use the full path if needed
lines = file.readlines()
# Parse lines into a DataFrame
data = []
for line in lines:
parts = line.strip().split()
if len(parts) == 13:
year = int(parts[0])
monthly_values = [float(val.replace('+', '')) for val in parts[1:]]
for month, value in enumerate(monthly_values, 1):
data.append({'Date': pd.Timestamp(year=year, month=month, day=1), 'SOI': value})
soi_df = pd.DataFrame(data)
# ✅ Filter from 1960 onwards
soi_recent = soi_df[soi_df['Date'] >= '1960-01-01']
# Plot the filtered data
plt.figure(figsize=(16, 5.5))
plt.plot(soi_recent['Date'], soi_recent['SOI'], linewidth=1)
# plt.title('Southern Oscillation Index (Monthly Averages, 1960–Present)')
plt.xlabel('Date')
plt.ylabel('SOI')
plt.grid(True)
plt.tight_layout()
plt.show()
# Load the Google stock data
gafa_df = pd.read_csv("data/gafa_stock.csv")
# Convert date column to datetime
gafa_df['ds'] = pd.to_datetime(gafa_df['ds'])
# Filter for Google's daily closing prices up to mid-2015
google_df = gafa_df[gafa_df['unique_id'] == 'GOOG_Close']
google_filtered = google_df[google_df['ds'] <= '2015-06-30']
# Plot the time series
plt.figure(figsize=(14, 5.5))
plt.plot(google_filtered['ds'], google_filtered['y'], linewidth=1)
# plt.title("Google Daily Closing Stock Price (Start – Mid-2015)")
plt.xlabel("Date")
plt.ylabel("Closing Price (USD)")
plt.grid(True)
plt.tight_layout()
plt.show()
# Compute the first differences of the closing prices
google_filtered['Price_Diff'] = google_filtered['y'].diff()
# Plot the first differences
plt.figure(figsize=(14, 5.5))
plt.plot(google_filtered['ds'], google_filtered['Price_Diff'], linewidth=1)
# plt.title("First Differences of Google Daily Closing Stock Price (Start – Mid-2015)")
plt.xlabel("Date")
plt.ylabel("Price Change (USD)")
plt.grid(True)
plt.tight_layout()
plt.show()
# Generate white noise with the same length as the Google filtered dataset
np.random.seed(42)
white_noise = np.random.normal(loc=0, scale=1, size=len(google_filtered))
# Create a time series with the same dates
white_noise_df = pd.DataFrame({
'Date': google_filtered['ds'].values,
'White_Noise': white_noise
})
# Plot the white noise series
plt.figure(figsize=(14, 5.5))
plt.plot(white_noise_df['Date'], white_noise_df['White_Noise'], linewidth=1)
# plt.title("Simulated White Noise Series (Same Length as Google Data)")
plt.xlabel("Date")
plt.ylabel("Value")
plt.grid(True)
plt.tight_layout()
plt.show()
from statsmodels.tsa.arima_process import ArmaProcess
# Define stationary AR(2) coefficients: (1 - 0.5L + 0.3L^2)
ar_params_stationary = np.array([1, -0.5, 0.3])
ma_params_stationary = np.array([1])  # No MA component
# Create and simulate the AR(2) process
ar2_stationary_process = ArmaProcess(ar=ar_params_stationary, ma=ma_params_stationary)
ar2_stationary_series = ar2_stationary_process.generate_sample(nsample=300, scale=1.0, burnin=100, distrvs=np.random.normal)
# Plot the stationary AR(2) series
plt.figure(figsize=(5.5, 5.5))
plt.plot(ar2_stationary_series, linewidth=1)
# plt.title("Simulated Stationary AR(2) Series (ϕ₁ = 0.5, ϕ₂ = -0.3)")
plt.xlabel("Time")
plt.ylabel("Value")
plt.grid(True)
plt.tight_layout()
plt.show()
# Define MA(2) coefficients: MA polynomial is (1 + 1.5L - 0.56L^2)
ar_params = np.array([1])  # No AR component
ma_params = np.array([1, 1.5, -0.56])
# Create the MA process and simulate
ma2_process = ArmaProcess(ar=ar_params, ma=ma_params)
ma2_series = ma2_process.generate_sample(nsample=300, scale=1.0, distrvs=np.random.normal)
# Plot the MA(2) series
plt.figure(figsize=(5.5, 5.5))
plt.plot(ma2_series, linewidth=1)
# plt.title("Simulated MA(2) Series (θ₁ = 1.5, θ₂ = -0.56)")
plt.xlabel("Time")
plt.ylabel("Value")
plt.grid(True)
plt.tight_layout()
plt.show()
# Define stationary AR(2) coefficients: (1 - 0.5L + 0.3L^2)
ar_params_stationary = np.array([1, -0.5, 0.3])
ma_params_stationary = np.array([1])  # No MA component
# Create and simulate the AR(2) process
ar2_stationary_process = ArmaProcess(ar=ar_params_stationary, ma=ma_params_stationary)
ar2_stationary_series = ar2_stationary_process.generate_sample(nsample=300, scale=1.0, burnin=100, distrvs=np.random.normal)
# Plot the stationary AR(2) series
plt.figure(figsize=(10, 5))
plt.plot(ar2_stationary_series, linewidth=1)
# plt.title("Simulated Stationary AR(2) Series (ϕ₁ = 0.5, ϕ₂ = -0.3)")
plt.xlabel("Time")
plt.ylabel("Value")
plt.grid(True)
plt.tight_layout()
plt.show()
# Create lagged versions of the SOI series
soi_df['Lag1'] = soi_df['SOI'].shift(1)
soi_df['Lag2'] = soi_df['SOI'].shift(2)
# Drop missing values
soi_lagged = soi_df.dropna()
# Calculate correlations
corr_lag1 = soi_lagged['SOI'].corr(soi_lagged['Lag1'])
corr_lag2 = soi_lagged['SOI'].corr(soi_lagged['Lag2'])
# Plot lag 1 scatterplot
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.scatterplot(x='Lag1', y='SOI', data=soi_lagged, alpha=0.5)
plt.title(f"Lag 1 Scatterplot\nCorrelation = {corr_lag1:.2f}")
plt.xlabel("SOI (Lag 1)")
plt.ylabel("SOI")
# Plot lag 2 scatterplot
plt.subplot(1, 2, 2)
sns.scatterplot(x='Lag2', y='SOI', data=soi_lagged, alpha=0.5)
plt.title(f"Lag 2 Scatterplot\nCorrelation = {corr_lag2:.2f}")
plt.xlabel("SOI (Lag 2)")
plt.ylabel("SOI")
plt.tight_layout()
plt.show()
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
# Filter the SOI series from 1960 onwards
soi_1960_onward = soi_df[soi_df['Date'] >= '1960-01-01'].dropna()
# Plot ACF separately
# plt.figure(figsize=(12, 4))
plot_acf(soi_1960_onward['SOI'], lags=40)
plt.title("ACF of SOI (1960–Present)")
plt.tight_layout()
plt.show()
# Plot PACF separately
# plt.figure(figsize=(12, 4))
plot_pacf(soi_1960_onward['SOI'], lags=40, method='ywm')
plt.title("PACF of SOI (1960–Present)")
plt.tight_layout()
plt.show()
# Load the uploaded CSV file
egypt_exports_df = pd.read_csv("data/egypt_exports.csv")
# Plot the time series of exports
plt.figure(figsize=(12, 5.5))
plt.plot(egypt_exports_df['ds'], egypt_exports_df['Exports'], marker='o', linestyle='-')
# plt.title("Exports from Egypt (Arab Rep.) Over Time")
plt.xlabel("Year")
plt.ylabel("Exports (% of GDP)")
plt.grid(True)
plt.tight_layout()
plt.show()
# Plot ACF with lags=20
plot_acf(egypt_exports_df['Exports'], lags=20)
# plt.title("ACF of Egypt's Exports (% of GDP)")
plt.tight_layout()
plt.show()
# Plot PACF with lags=20
plot_pacf(egypt_exports_df['Exports'], lags=20, method='ywm')
# plt.title("PACF of Egypt's Exports (% of GDP)")
plt.tight_layout()
plt.show()
from statsmodels.tsa.arima.model import ARIMA
import numpy as np
# Prepare the data
y = egypt_exports_df['Exports'].dropna().values
# Define range of parameters for grid search
p_values = range(0, 4)
d_values = range(0, 2)
q_values = range(0, 4)
# Perform grid search based on AIC
best_aic = np.inf
best_order = None
best_model = None
for p in p_values:
for d in d_values:
for q in q_values:
try:
model = ARIMA(y, order=(p, d, q)).fit()
if model.aic < best_aic:
best_aic = model.aic
best_order = (p, d, q)
best_model = model
except Exception:
continue
# best_order, best_aic
require(ISLR)
install.packages(ISLR)
install.packages("ISLR2")
library(ISLR2)
Default
write.csv(Default, file = "8_logisitic_regression/data/Default.csv")
reticulate::repl_python()
