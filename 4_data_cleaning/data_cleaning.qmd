---
title: "Understanding Data"
format:
  revealjs:
    theme: [default, ../assets/custom.scss]
    footer: "Data Science Transforming Maintenance"
    logo: ../assets/logo.png
    slide-number: true
    show-side-number: all
    include-after-body: ../assets/clean_title_page.html
    echo: true
title-slide-attributes:
  data-background-image: ../assets/title_background.png
  data-background-size: contain
  data-background-opacity: "1.0"
jupyter: python3
---

## Outline

1. Understand both measurement and systematic errors in a small dataset
2. Know how to clean data in Python
3. Be able to combine data sets using:
    * 'Vertical' concatenation
    * 'Horizontal' merging
4. Resample and aggregate time series data on different intervals


## Cleaning data {.smaller}

Consider the following timeseries data set on windspeed readings:

```{python}
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(12348)

xs1 = np.linspace(0, 1)
ys1 = 0.5 * xs1 + 1 * xs1**2

xs2 = np.linspace(1, 3)
ys2 = -3.5 + 7 * xs2 + 0.5 * (xs2 - 1)**2 - 1.3 * (xs2 - 1)**3

xs = np.concatenate([xs1, xs2])
ys = np.concatenate([ys1, ys2])

ys[19] = 23
ys[76] = 21

# xs = np.linspace(-0.25, 2, 100)
# ys = 5 + 10 * xs - 25 * xs**2 + 21 * xs**3 - 5 * xs**4 + 0.5 * np.sin(4 * xs)

ys = ys * np.exp(1j * np.sin(1.5* xs))

ys[80:] *= np.exp(1j * np.pi / 2)


ys += 1 * (np.random.randn(100) + 1j * np.random.randn(100))

df = pd.DataFrame({
  "Time": xs,
  "Windspeed": np.abs(ys),
  "Direction": np.rad2deg(np.angle(ys))
})
None
```

::: {.panel-tabset}
### Table

```{python}
#| echo: false
df
```

### Plot

```{python}
# | echo: false

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 3.7))

ax1.scatter(df.Time, df.Windspeed, s=10)
ax1.set_ylim(0, 25)
ax1.grid(visible=True, axis="y", linestyle="dashed")
ax1.set_ylabel("Windspeed [m/s]")
ax1.set_xlabel("Time [hours]")

idx = df.Windspeed > 20
for idx, row in df[idx].iterrows():
    ax1.annotate(
        "",
        (row.Time, row.Windspeed),
        xytext=(25, -25),
        textcoords="offset points",
        arrowprops={
            "color": "red",
            "width": 1,
            "headwidth": 7,
            "headlength": 7,
            "shrink": 0.1,
        },
    )

ax2.scatter(df.Time, df.Direction, s=10)
ax2.set_ylim(-180, 180)
ax2.set_yticks([-180, -90, 0, 90, 180])
ax2.set_ylabel("Direction [deg]")
ax2.set_xlabel("Time [hours]")
ax2.grid(visible=True, axis="y", linestyle="dashed")

t = df.iloc[[79, 80], 0].mean()
ax2.vlines(t, *df.iloc[[79, 80]]["Direction"], color="red")
None
```

### BoxPlot

::: {.columns}

::: {.column width="50%"}

A **box plot** provides a visual guide to the distribution and skew of a data set:

* The lower and upper edges of the box indicate the 1st and 3rd **quartiles**
* The central (orange) line indicates the **median**
* The whiskers indicate a multiple of the **interquartile range** (in this case: 3x)
* **Outliers** beyond this range are indicated by individual points


:::

::: {.column width="50%"}

```{python}
# | echo: false

fig, ax = plt.subplots(figsize=(6, 4))

winds = [df.iloc[rowstart : (rowstart + 20), 1] for rowstart in range(0, len(df), 20)]
midtimes = [
    f"{df.iloc[rowstart : (rowstart + 20), 0].mean():.1f}"
    for rowstart in range(0, len(df), 20)
]

ax.boxplot(winds, tick_labels=midtimes, whis=3)
ax.set_xlabel("Time [hours]")
ax.set_ylabel("Windspeed [m/s]");

```

:::

:::

### Comments

* Two prominent high-speed outliers indicate gross errors
  * Planes taking off at these times caused wake turbulence
* A sudden break in windspeed direction at ~2.25 h suggests a systematic error
  * The anemometer was accidentally rotated 90 degrees by ground staff
* The highly variable wind directions in the first hour _are not_ errors: low-windspeed directions are highly variable


:::

## Cleaning data: excising data {.smaller}

In Pandas, setting a row as `None` or `NaN` (not a number) will mark the row as missing.

```{python}
# Mark outlier rows as invalid:
df[df.Windspeed > 20] = None # could also use: np.nan
print("Initial rows:", len(df))

# Remove rows containing missing data:
df = df.dropna()
print("Final rows:", len(df))
```


```{python}
# | echo: false

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 3.7))

ax1.scatter(df.Time, df.Windspeed, s=10)
ax1.set_ylim(0, 25)
ax1.grid(visible=True, axis="y", linestyle="dashed")
ax1.set_ylabel("Windspeed [m/s]")
ax1.set_xlabel("Time [hours]")

ax2.scatter(df.Time, df.Direction, s=10)
ax2.set_ylim(-180, 180)
ax2.set_yticks([-180, -90, 0, 90, 180])
ax2.set_ylabel("Direction [deg]")
ax2.set_xlabel("Time [hours]")
ax2.grid(visible=True, axis="y", linestyle="dashed")
None
```

## Cleaning data: correcting {.smaller}

::: {.columns style="font-size: 0.9em;"}

::: {.column width="50%"}

1. Identify the location of the break:

```{python}
df.query("2.1 < Time < 2.3")
```

:::
::: {.column width="50%"}

2. Rotate the direction data back into place:

```{python}
df.Direction.loc[80:] -= 90
```

:::
:::

```{python}
# | echo: false

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 3.7))

ax1.scatter(df.Time, df.Windspeed, s=10)
ax1.set_ylim(0, 25)
ax1.grid(visible=True, axis="y", linestyle="dashed")
ax1.set_ylabel("Windspeed [m/s]")
ax1.set_xlabel("Time [hours]")

ax2.scatter(df.Time, df.Direction, s=10)
ax2.set_ylim(-180, 180)
ax2.set_yticks([-180, -90, 0, 90, 180])
ax2.set_ylabel("Direction [deg]")
ax2.set_xlabel("Time [hours]")
ax2.grid(visible=True, axis="y", linestyle="dashed")
None
```


## Concatenating data {.smaller}

```{python}
#| echo: false

from pathlib import Path
import pandas as pd

dfs = []
for name in ["IDCJDW6111.202502.csv", "IDCJDW6111.202503.csv", "IDCJDW6111.202504.csv"]:
    dfs.append(
          pd.read_csv(
              Path("data") / name,
              skiprows=8,
              header=0,
              parse_dates=["Date"],
              index_col="Date",
              encoding="latin_1"
          ).iloc[:, 1:]
    )
```

The simplest way to combine DataFrames is by stacking data **horizontally** using `concat([df1, ...])`. If columns match, they will be fused; if not, they will be filled with missing values.

::: {.panel-tabset}

### March

```{python}
#| echo: false
df1 = dfs[1].iloc[:, [0, 1, 2, 3]]
from IPython.display import display
with pd.option_context('display.max_rows', 4):
  display(df1)
```

### April

```{python}
#| echo: false
df2 = dfs[2].iloc[:, [0, 1, 4]]
from IPython.display import display
with pd.option_context('display.max_rows', 4):
  display(df2)
```

### Combined

```{python}
# Any number of DataFrames can be combined, e.g. pd.concat([df1, df2, df3, df4, ...])
dfcombined = pd.concat([df1, df2])
```

```{python}
#| echo: false
from IPython.display import display
with pd.option_context('display.max_rows', 4):
  display(dfcombined)
```

:::

## Merging data {.smaller}

DataFrames can also be combined **horizontally** using `.merge()`.

::: {style="font-size: 0.8em; "}

```{python}
#| echo: false
dfcombined = pd.concat(dfs)
df1 = dfcombined.iloc[:, [0, 1]]
df2 = dfcombined.iloc[:, [2, 3]]
```

:::

::: {.panel-tabset}

### Two related tables

* Independent daily weather recordings over the same date range.

::: {.columns style="font-size: 0.8em;"}

::: {.column width="50%"}

```{python}
#| echo: false
from IPython.display import display
with pd.option_context('display.max_rows', 4):
  display(df1)
```

:::
::: {.column width="50%"}

```{python}
#| echo: false
from IPython.display import display
with pd.option_context('display.max_rows', 4):
  display(df2)
```

:::
:::

::: {.callout-note}

For those familiar with SQL, this operation is akin to `JOIN`.

:::

### Merged

* An **inner** join restricts the merged table to only include rows that exist in _both_ tables.

```{python}
# SQL equivalent: SELECT * FROM df1, df2 INNER JOIN ON df1.Date = df2.Date
dfcombined = pd.merge(df1, df2, how="inner", on="Date")
```

```{python}
#| echo: false
from IPython.display import display
with pd.option_context('display.max_rows', 4):
  display(dfcombined)
```

:::

## Merging data {.smaller}

::: {style="display: flex;"}

* A more common example is where DataFrames reference a key value used elsewhere.
* In this example, we have an equipment hire log, but the users and equipment parts are kept in other tables.
* We use a `LEFT JOIN` to collate all the data into a new table.


::: {style="text-align: center; font-size: 0.6em; margin-top: -2em;"}

![](images/merge.png){width=600}
[(Source)](https://grundideefciwire.z21.web.core.windows.net/sql-join-venn-diagrams.html)

:::


:::

```{python}
#| echo: false
users = pd.DataFrame({
  "UserID": [1452, 532, 5426],
  "FirstName": ["Bart", "Moe", "Lindsey"],
  "LastName": ["Simpson", "Szyslak", "Naegle"],
  "Occupation": ["Student", "Bartender", "PR Consultant"]
}).set_index("UserID")

parts = pd.DataFrame({
  "SerialID": [124, 432, 435],
  "Description": ["Slingshot", "Beer Keg", "Cellular tower"]
}).set_index("SerialID")

hirelog = pd.DataFrame(
    {
        "Date": ["2025/01/13", "2025/02/02", "2025/03/12", "2025/04/14"],
        "UserID": [1452, 5426, 532, 532],
        "SerialID": [124, 435, 432, 432],
        "State": ["HIRED", "HIRED", "HIRED", "RETURNED"],
    }
).set_index("Date")

```


::: {.panel-tabset}

### Three related tables

::: {.columns}
::: {.column width="40%"}

#### Users {style="text-align: center;"}

```{python}
#| echo: false
display(users)
```

:::
::: {.column width="20%"}

#### Parts {style="text-align: center;"}
```{python}
#| echo: false
display(parts)
```
:::
::: {.column width="35%"}
#### Hire Log {style="text-align: center;"}
```{python}
#| echo: false
display(hirelog)
```
:::
:::

### Merged

::: {style="font-size: 0.8em;"}

```{python}
(
    hirelog.reset_index()  # Temporarily demote the Date index to a regular column
    .merge(users, how="left", on="UserID", validate="many_to_one")  # Merge the users table
    .merge( parts, how="left", on="SerialID", validate="many_to_one")  # Merge the parts table
    .set_index("Date")  # Set the Date column back as index
)
```

:::

:::

## Data on different time scales {.smaller}

```{python}
# | echo: false

from pathlib import Path

dfs = []

for fname in [
    "IDCJDW6111.202502.csv",
    "IDCJDW6111.202501.csv",
    "IDCJDW6111.202503.csv",
    "IDCJDW6111.202504.csv",
]:
    dfs.append(
        pd.read_csv(
            Path("data") / fname,
            skiprows=8,
            encoding="latin_1",
            index_col="Date",
            parse_dates=["Date"],
        ).iloc[:, [1, 2, 3]]
    )

df = pd.concat(dfs)
df.index = df.index.to_period("D")
# display(df)


dfweekly = (
    df.resample("7D", label="left")
    .agg(
        {
            "Minimum temperature (°C)": "min",
            "Maximum temperature (°C)": "max",
            "Rainfall (mm)": "sum",
        }
    )
    .query("Date < '2025/03/05'")
    .rename_axis("Week starting")
)

dfdaily = df.query("Date >= '2025/03/01'")

dfweekly.index = dfweekly.index.to_timestamp()
dfdaily.index = dfdaily.index.to_timestamp()
```

::: {.columns}

::: {.column width="50%"}

* Consider the following two data sets:
    * One provides weekly data for the first 9 weeks of 2025
    * The other provides daily data for March and April
* To merge these datasets we need to **resample** the data onto a common frequency.

:::
::: {.column width="50%"}

```{python}
# | echo: false
fig, ax = plt.subplots(figsize=(6, 4))
ax.set_title("Temperature ranges (min - max)")
ax.fill_between(
    dfdaily.index, dfdaily.iloc[:, 0], dfdaily.iloc[:, 1], alpha=0.2, color="tab:blue"
)
ax.fill_between(
    [*dfweekly.index, dfdaily.index[0]],
    [*dfweekly.iloc[:, 0], dfweekly.iloc[0, 0]],
    [*dfweekly.iloc[:, 1], dfweekly.iloc[0, 1]],
    alpha=0.2,
    color="tab:blue",
)

ax.scatter(dfdaily.index, dfdaily.iloc[:, 0], color="tab:blue", marker="D", s=5)
ax.scatter(dfdaily.index, dfdaily.iloc[:, 1], color="tab:blue", marker="D", s=5)
ax.scatter(dfweekly.index, dfweekly.iloc[:, 0], color="tab:blue", marker="D", s=5)
ax.scatter(dfweekly.index, dfweekly.iloc[:, 1], color="tab:blue", marker="D", s=5)

ax.axvline(
    dfdaily.index[0], 0, 1, linestyle="dashed", color="black", zorder=-1000, alpha=0.5
)
ax.set_ylabel("Temperature [deg]")
ax.set_xlabel("Date")
ax.set_xticks(
    [
        pd.to_datetime(x)
        for x in ("2025/01/01", "2025/02/01", "2025/03/01", "2025/04/01", "2025/05/01")
    ]
)

```

:::
:::

::: {.columns style="font-size: 0.7em;"}

::: {.column width="50%"}

```{python}
#| echo: false
from IPython.display import display
with pd.option_context('display.max_rows', 4):
  display(dfweekly)
```

:::

::: {.column width="50%"}

```{python}
# | echo: false
from IPython.display import display
with pd.option_context("display.max_rows", 4):
  display(dfdaily)
```

:::

:::

## Downsampling {.smaller}

::: {.columns}

::: {.column width="50%"}

* We can reduce the frequency of the daily data (**downsample**) to match the weekly timesteps
* Each measurement will need to be downsampled differently:
  * Minimum (maximum) temperature will be the minimum (maximum) value of all values occurring in the week
  * Total rainfall will need to be the _sum_ of the daily rainfall
* In Pandas we use `resample()` to do this

:::

::: {.column width="50%"}

```{python}
# | code-fold: true
# | code-summary: "Show the code"
dfmerged = pd.concat(
    [
        dfweekly,
        dfdaily.query("Date >= '2025/03/05'")
        .resample("7D")
        .agg(
            {
                "Minimum temperature (°C)": "min",
                "Maximum temperature (°C)": "max",
                "Rainfall (mm)": "sum",
            }
        )
        .rename_axis("Week starting"),
    ]
).sort_index()
```


```{python}
# | echo: false

fig, ax = plt.subplots(figsize=(6, 4))
ax.fill_between(dfmerged.index, dfmerged.iloc[:, 0], dfmerged.iloc[:, 1], alpha=0.2)
ax.scatter(dfmerged.index, dfmerged.iloc[:, 0], color="tab:blue", s=5, marker="D")
ax.scatter(dfmerged.index, dfmerged.iloc[:, 1], color="tab:blue", s=5, marker="D")

ax.set_ylabel("Temperature [deg]")
ax.set_xlabel("Date")
ax.set_xticks(
    [
        pd.to_datetime(x)
        for x in ("2025/01/01", "2025/02/01", "2025/03/01", "2025/04/01", "2025/05/01")
    ]
)

```

:::

:::

## Upsampling {.smaller}

::: {.columns}

::: {.column width="50%"}

* Let's suppose we need the data on a daily frequency. We can **upsample** the data.
* This requires _interpolating_ the missing data somehow:
  * For example, we can linearly interpolate the minimum and maximum temperatures
  * Is this reasonable?

:::

::: {.column width="50%"}

```{python}
#| code-fold: true
#| code-summary: "Show the code"

dfmerged = pd.concat(
    [
        dfdaily,
        dfweekly.resample("1D")
        .interpolate("time")
        .rename_axis("Date")
        .query("Date < '2025/03/01'"),
    ]
).sort_index()
```

```{python}
#| echo: false
fig, ax = plt.subplots(figsize=(6, 4))
ax.fill_between(dfmerged.index, dfmerged.iloc[:, 0], dfmerged.iloc[:, 1], alpha=0.2)
ax.scatter(dfmerged.index, dfmerged.iloc[:, 0], color="tab:blue", s=3, marker="D")
ax.scatter(dfmerged.index, dfmerged.iloc[:, 1], color="tab:blue", s=3, marker="D")

ax.set_ylabel("Temperature [deg]")
ax.set_xlabel("Date")
ax.set_xticks(
    [
        pd.to_datetime(x)
        for x in ("2025/01/01", "2025/02/01", "2025/03/01", "2025/04/01", "2025/05/01")
    ]
)

```

:::

:::

## Key points {.smaller}

1. Data can have different types of errors:
    * Measurement errors are random (normal) errors that arise from the measurement process
    * Systematic errors are errors that introduce a consistent bias, skew, offset or multiplier into your data
2. You can clean or correct data using standard DataFrames methods
3. Data can be merged in two ways:
    * Data can be merged vertically using `pd.concat([df1, df2, ...])`
    * Data can be merged horizontally using `df1.merge(df2, left_on="key1", right_on=key2)`
4. Time series data can be resampled to new time frequencies using `df.resample(freq)`
