---
title: "Linear Regression"
format:
  revealjs:
    theme: [default, ../assets/custom.scss]
    footer: "Data Science Transforming Maintenance"
    logo: ../assets/logo.png
    menu: true
    slide-number: true
    show-side-number: all
    number-sections: true
    number-depth: 1
    include-after-body: ../assets/clean_title_page.html
    echo: false
    # argument for reveal-header
    sc-sb-title: true
title-slide-attributes:
  data-background-image: ../assets/title_background.png
  data-background-size: contain
  data-background-opacity: "1.0"
# reveal-header extension
filters: 
  - reveal-header
# argument for reveal-header  
slide-level: 2    
jupyter: python3
---

```{python}
#| echo: False
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import statsmodels.formula.api as smf
import pandas as pd
from matplotlib.patches import Rectangle
import os
```

## Outline


1. Introduction to linear regression
2. Fitting regression models to data
3. Interpreting regression models
4. Assessing goodness-of-fit
5. Making predictions from regression models


# Introduction to linear regression

## Why linear models?

::: incremental
* We often fit linear models because:
  + Sometimes the underlying relationship is _approximately_ linear
  + A simple model might be "good enough" for our purposes
  + They _might_ provide a good approximation to nonlinear relationships over a narrow region
  + It often makes sense to check first if a linear relationship fits; if it doesn't, we can fit more complex models
:::

## Why linear models?

::: {.callout-note icon=false}
## Remember ...

_All models are wrong, but some are useful._

G.E.P. Box, 1920--2013
:::

## Example: estimating viscosity {.smaller}

::: incremental
* From theory, the slope of the relationship between shear stress $\tau$ and shear rate $\gamma$ is the viscosity of the fluid $\eta_1$
$$
\tau = \eta_0 + \eta_1 \gamma
$$
* Because of variability and noise (instrumental, raw material), the points are scattered about a straight line
* We postulate a statistical model for the observed data
$$
\tau_i = \eta_0 + \eta_1 x_i + \epsilon_i,\,\,\, \epsilon_i \sim N(0, \sigma^2)
$$
* The parameters $\eta_0$ and $\eta_1$ are constants whose values have physical meaning and that we want to estimate
:::

## Example: estimating viscosity {.smaller}

:::: {.columns}

::: {.column width="60%"}
```{python}

# Step 0: Set seed
np.random.seed(8783)

# Step 1: Generate 20 random points between 1 and 10
x = np.random.uniform(1, 10, 20)

# Step 2: Create the variable y by multiplying x by 3 and adding Gaussian noise
y = 3 * x + np.random.normal(0, 2, 20)
```

```{python}

X = sm.add_constant(x)
model = sm.OLS(y, X)
results = model.fit()
```


```{python}

# Plot with the updated axis labels including SI units
plt.figure(figsize=(6, 6))
plt.scatter(x, y, color='#34643B', s=75, marker='o')  # Filled circles with reduced size
plt.plot(x, results.fittedvalues, color="black")  # Black regression line
plt.xlabel("shear rate (γ, 1/s)")  # X-axis label with Greek gamma and SI unit
plt.ylabel("shear stress (τ, Pa)")  # Y-axis label with Greek tau and SI unit
plt.grid(True)
xl = plt.xlim(0, None)
yl = plt.ylim(0, None)
plt.tight_layout()
plt.show()
```
:::

::: {.column width="40%"}
* After carrying out an experiment, we want to estimate the parameters and the associated uncertainty
* Using least squares, the estimate of the slope ($\eta_1$), the viscosity is
$$
\widehat{\eta}_1 =`{python} results.params[1].round(2)`\,\,\mbox{PI}
$$
with a 95% confidence interval of
$$
(`{python} list(map(float, results.conf_int(alpha = 0.05)[1].round(2)))`)
$$
:::

::::

## Example: modelling pipe degradation {.smaller}

:::: {.columns}

::: {.column width="50%"}
![](images/pipe_x-section.png)
:::

::: {.column width="50%"}
* There are many instances where we may fit a linear model because the observed relationship _appears_ to be linear
* Consider, for example, measurements of the wall thickness of pipe spools measured over time using an ultrasonic thickness gauge
* Should we expect degradation to be linear?
* Can we predict when the thickness will reach the critical threshold?
:::

::::

## Example: modelling pipe degradation {.smaller}

:::: {.columns}

::: {.column width="60%"}
```{python}

# Step 0: Set seed
np.random.seed(8339)

# Step 1: Generate 17 equally spaced points between zero and 17
x = np.linspace(0, 8, 17)

# Step 2: Draw degradation rates from normal distribution
deg_rates = np.random.normal(-1, .1, 4)
deg_rates = deg_rates[[1, 0, 3, 2]]

# Step 3: Create design matrix and then multiply by degradation rates
#         Also add the initial thickness
Xb = 15 + x.repeat(4).reshape(17, 4) * deg_rates

# Step 2: Create the variable y by multiplying x by -0.733 and adding Gaussian noise
#         Start with an initial thickness of 15 mm
Y = Xb + np.random.normal(0, 1.1, 68).reshape(17, 4)
Y[0] = [15, 15, 15, 15]
```

```{python}

# Plot with the updated axis labels including SI units
plt.figure(figsize=(6, 6))
# plt.scatter(x, y, color='#34643B', s=50, marker='o')  # Filled circles with reduced size
plt.plot(x, Y, label = ["A1", "A2", "A3", "A4"])
# plt.plot(x, results.fittedvalues, color="black")  # Black regression line
plt.xlabel("time")
plt.ylabel("thickness (mm)")
plt.grid(True)
xl = plt.xlim(-0.20, 10)
yl = plt.ylim(0, None)
plt.legend(loc = "upper right")
plt.axline((0, 2), (10, 2), linestyle = "--", linewidth = 2, color = "black")
plt.tight_layout()
plt.show()
```
:::

::: {.column width="40%"}
* Plot shows degradation of pipe wall over time at each location
* Traces appear to be linear, but with considerable noise---measured degradation is not monotonically decreasing
* **What will the pipe thickness be at $t=10$?**
:::

::::

## Example: modelling pipe degradation {.smaller}

:::: {.columns}

::: {.column width="60%"}
```{python}
#| cache: true

#  Use pandas data frame and R syntax to fit model for quadrant A3
max_abs_deg = np.argmin(deg_rates) # index of greatest degradation rate
data = {
  'y': Y[:, 2], # pick 3rd column
  'x': x
}
df = pd.DataFrame(data)

df.to_csv('data/thickness.csv', header = ('thickness', 'time'), index = False)

model = smf.ols(formula = 'y ~ x', data = df)
model_f = model.fit()

# print(summarize(model_f)) # from ISLP, much more compact
# print(results.summary()) # too much detail
```

```{python}
#| cache: true

# Do the same, but extend the predictions out to where the soft threshold might be reached

# Generate new set of x-values beyond observed data and put in a data frame
df_new = pd.DataFrame({'x': np.linspace(0, 14, 29)})

predicted_values = model_f.get_prediction(df_new)
predicted_values_sf = predicted_values.summary_frame(alpha=0.05)
```

```{python}
#| cache: true

# Data points
ax = df.plot(kind='scatter', x='x', y='y', color='green', label='Data', figsize=(6, 6))
ax.plot(df['x'], df['y'], color='green')

# Predicted line
ax.plot(df_new['x'], predicted_values_sf['mean'], color='black', label='Regression Line')

# Prediction interval
ax.fill_between(df_new['x'], predicted_values_sf['obs_ci_lower'], predicted_values_sf['obs_ci_upper'], color='lightgrey', alpha=0.2, label='95% Prediction Interval')

ax.set(xlabel='time', ylabel='thickness (mm)')
xl = ax.set_ylim(0, 17.9)
yl = ax.set_ylim(-0.01, None)
ax.grid(visible = 'true')
ax.legend()
ax.axline((0, 2), (16, 2), linestyle = "--", linewidth = 2, color = "black")
plt.tight_layout()
plt.show()
```
:::

::: {.column width="40%"}
* For now, we consider modelling only the degradation trace at A3
* How can we find the 'line of best fit?'
* How can we use it to predict when the degradation might reach the soft threshold?
* How should we calculate the uncertainty in any predictions we make?
:::

::::

# Fitting regression models

## Model and terminology {.smaller}
* We have $n$ pairs of observations
$$
(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)
$$
* The $x_i$ are the values of the **explanatory** or **predictor** variable, and the $y_i$ denote the values of the **response** variable
* We distinguish between $x$ and $y$ because we might want to:
  + predict $y$ from $x$
  + explain the variability in $y$ by $x$
  + control $y$ using $x$

## Model and terminology {.smaller}
* Simple linear regression model is written as
$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i,\,\,\,\,i=1, 2, \ldots, n,
$$
which consists of the 'fixed' or 'deterministic' part $\beta_0 + \beta_1 x$ and a 'random' part $\epsilon$ that is used to model the scatter (the 'residuals') about the underlying relationship
* Given pairs of observations $(x_i, y_i)$, the problem is to find estimates of the **parameters** $\beta_0$ and $\beta_1$ that defines the 'line of best fit'
* Quantities estimated from the data are denoted by putting a hat ($\hat{}$) over them, e.g., $\widehat{\beta}_0$, $\widehat{\beta}_1$, etc.
* Estimates are found using the **method of least squares**, invented by Legendre (1805) and Gauss (1809)!
* Once the parameters have been estimated, **and the model fit has been assessed**, we can then, e.g., use the model for making predictions

## Least squares: how it works {.smaller #least-squares}
* Least squares is a geometric criterion: we find the line, that of all possible lines, minimizes the sum of the squares of the vertical distances from the data points to the fitted line
* The fitted value of the response variable at $x_i$ is denoted $\widehat{y}_i$, and it is calculated as
$$
\widehat{y}_i = \widehat{\beta}_0 + \widehat{\beta}_1 x_i
$$
* The differences are the **residuals** $\widehat{\epsilon}_i = y_i - \widehat{y}_i$
* To find the line that 'best fits' the data, we minimize the **residual sum of squares** ($\mbox{RSS}$), the sum of squared differences between $y_i$ and $\widehat{y}_i$:
$$
\mbox{RSS} = \sum_{i=1}^n \widehat{\epsilon}_i^2 = \sum_{i=1}^n (y_i - \widehat{y}_i)^2 = \sum_{i=1}^n (y_i - \widehat{\beta}_0 - \widehat{\beta}_1 x_i)^2
$$

## Least squares: which fit is better?

:::: {.columns}

::: {.column width="50%" .fragment}
```{python}
#| cache: true

# Now let's plot the LS fit along with vertical lines joining the data and the fitted values.

# Data points
ax = df.plot(kind='scatter', x='x', y='y', color='green', label='Data', figsize=(6, 6))
# Regression line
ax.plot(df['x'], model_f.fittedvalues, color='black', label='Regression Line')

ax.vlines(df['x'], df['y'], model_f.fittedvalues, color = 'grey')

ax.set(xlabel='time', ylabel='thickness (mm)')
ax.grid(visible = 'true')
ax.legend()
plt.tight_layout()
plt.show()
```
:::

::: {.column width="50%" .fragment}
```{python}
#| cache: true

# Now let's do the same for another (not OLS) line

y_new = model_f.params['Intercept'] + (model_f.params['x']/1.3) * df['x']
# Data points
ax = df.plot(kind='scatter', x='x', y='y', color='green', label='Data', figsize=(6, 6))
# Regression line
ax.plot(df['x'], y_new, color='black', label='Regression Line')

ax.vlines(df['x'], df['y'], y_new, color = 'grey')

ax.set(xlabel='time', ylabel='thickness (mm)')
ax.grid(visible = 'true')
ax.legend()
plt.tight_layout()
plt.show()
```
:::

::::

## Least squares: which fit is better?

:::: {.columns}

::: {.column width="50%" }
```{python}
#| cache: true

# Now let's plot the LS fit along with vertical lines joining the data and the fitted values and add rectangles

# Data points
ax = df.plot(kind='scatter', x='x', y='y', color='green', label='Data', figsize=(6, 6))
# Regression line
ax.plot(df['x'], model_f.fittedvalues, color='black', label='Regression Line')

ax.vlines(df['x'], df['y'], model_f.fittedvalues, color = 'grey')

xa = ax.set_xlim(-1, None)
ax.set(xlabel='time', ylabel='thickness (mm)')
ax.grid(visible = 'true')
ax.legend()

# Loop to plot rectangles
n = len(x)
fittedvalues = model_f.fittedvalues
resid = model_f.resid
for i in range(0,n):
  lx = x[i]
  ly = fittedvalues[i]
  rd = resid[i]
  plt.gca().add_patch(Rectangle((lx, ly), rd, rd, fill = 'true', color = 'forestgreen', alpha = 0.25))

plt.tight_layout()
plt.show()
```
:::

::: {.column width="50%" }
```{python}
#| cache: true

# Now let's do the same for another (not OLS) line and add rectangles (squares)

y_new = model_f.params['Intercept'] + (model_f.params['x']/1.3) * df['x']
resid_new = df['y'] - y_new
# Data points
ax = df.plot(kind='scatter', x='x', y='y', color='green', label='Data', figsize=(6, 6))
# Regression line
ax.plot(df['x'], y_new, color='black', label='Regression Line')

ax.vlines(df['x'], df['y'], y_new, color = 'grey')

xa = ax.set_xlim(-1, None)
ax.set(xlabel='time', ylabel='thickness (mm)')
ax.grid(visible = 'true')
ax.legend()

# Loop to plot rectangles
n = len(x)
for i in range(0,n):
  lx = x[i]
  ly = y_new[i]
  rd = resid_new[i]
  plt.gca().add_patch(Rectangle((lx, ly), rd, rd, fill = 'true', color = 'forestgreen', alpha = 0.25))

plt.tight_layout()
plt.show()
```
:::

::::


## Least squares: assumptions
* To use least squares to estimate the parameters and the associated uncertainty requires us to make (**and then check**!) some assumptions:

  ::: {style="font-size: 80%;"}
  1. The response varies about the underlying regression line with a Normal distribution
  2. The standard deviation of the responses is the same at all values of $x_i$
  3. The underlying relationship between the response and the explanatory variables is linear
  4. The observations are independent
  :::


# Interpreting regression models

## Regression in Python {#OLS-code-output .smaller}

We use the package `statsmodels` and its Ordinary Least Squares (OLS) solver:

::: {.panel-tabset}

### Code 1

In method we have to build the matrix $\mathbf{X}$:

```{python}
# | echo: true
# | code-line-numbers: "1-100|2|4-5|7-8|10-11|1-100"
import pandas as pd
import statsmodels.api as sm

# Import .csv file
thick_data = pd.read_csv("data/thickness.csv")

# Construct X
X = sm.add_constant(thick_data["time"])  # Add a column of 1s

# Perform the fit: OLS(y, X).fit()
fit_thick_model = sm.OLS(thick_data["thickness"], X).fit()
```

### Code 2

* This method uses a very powerful `formula` submodule (see [here](https://www.statsmodels.org/stable/example_formulas.html))
* We model $y_\text{thickness} = \beta_0 + \beta_1 t$ using the syntax `thickness ~ time`

```{python}
# | echo: true
# | code-line-numbers: "1-100|2|7-8|1-100"
import pandas as pd
import statsmodels.formula.api as smf

# Import .csv file
thick_data = pd.read_csv("data/thickness.csv")

# Define model
fit_thick_model = smf.ols(formula="thickness ~ time", data=thick_data).fit()
```

### Output

```{python}
#| echo: true
# Print condensed summary
print(fit_thick_model.summary(slim = True))
```

### ANOVA

```{.python}
sm.stats.anova_lm(fit_thick_model)
```


```{python}
sm.stats.anova_lm(fit_thick_model)
```

:::

```{python}
beta1hat = fit_thick_model.params.iloc[1].round(2)
confint = fit_thick_model.conf_int()
confint_ll = confint.iloc[1][0].round(2)
confint_ul = confint.iloc[1][1].round(2)
R2 = fit_thick_model.rsquared.round(2)
```


## Interpreting the output: summary {.smaller}
* The least squares estimate of the degradation rate is
$\widehat{\beta}_1 =`{python} beta1hat`\,\mathsf{mm/unit~time}$, and the 95% confidence interval for $\beta_1$ is $(`{python} confint_ll`, `{python} confint_ul`)$. Because zero is not within the interval, there is clearly a relationship between thickness and time.

  ::: {style="font-size: 80%;"}
  - For moderate and large sample sizes, the approximation $\widehat{\beta}_1 \pm 2\times \mathsf{\mbox{std err}}$ is good enough
  :::

* The $t$-value tests the hypothesis $H_0: \beta_1 = 0$ against $H_1: \beta_1 \neq 0$; the $p$-value is so small that we would reject the null hypothesis in favour of the alternative

  ::: {style="font-size: 80%;"}
  - But we knew that already by looking at the confidence interval!
  :::

* The value of $R^2 = `{python} R2`$ is the fraction of the total variation in the data explained, or accounted for, by the regression

  ::: {style="font-size: 80%;"}
  - When there is only one explanatory variable $x$, $R^2$ is identical to the squared correlation coefficient between $x$ and the response $y$. This is **not** true when there are several explanatory variables.
  :::

## Interlude: sums of squares {.smaller}

::: {.panel-tabset}

### Decomposition

```{python}
#| cache: true

from curlyBrace import curlyBrace

# Set the seed for reproducibility
np.random.seed(4095894)

# Define the linear relationship and noise parameters
x = np.array([1, 2, 3, 4])
# Regenerate the y values with noise (std=0.9)
y_new_09_seeded = 1.5 * x + 0.25 + np.random.normal(0, 0.9, len(x))

# Calculate the mean of the seeded y values
mean_y_09_seeded = np.mean(y_new_09_seeded)

plt.scatter(x, y_new_09_seeded, color='brown')
plt.plot(np.append(x, 5), 1.5 * np.append(x, 5) + 0.25, color='red', linestyle='--')

# Draw vertical lines from each data point to the original line
for i, (xi, yi) in enumerate(zip(x, y_new_09_seeded)):
    y_original = 1.5 * xi + 0.25
    plt.vlines(xi, y_original, yi, color='blue', linestyle=':')
    if i == len(x) - 1:  # Only for the 4th point
        plt.text(xi + 0.4, (y_original + yi) / 2, r'$y_i - \widehat{y}_i$', color='gray', fontsize = 12)

# Draw a solid green line for the fourth point from the original line to the mean
plt.vlines(x[-1], 1.5 * x[-1] + 0.25, mean_y_09_seeded, color='blue')

# Add the label for the line between the original line and the mean
plt.text(x[-1] + 0.4, (1.5 * x[-1] + 0.25 + mean_y_09_seeded) / 2, r'$\widehat{y}_i - \bar{y}$', color='gray', fontsize = 12)

# Mean line for reference
plt.axhline(mean_y_09_seeded, color='gray', linestyle='-.')

# Add the label for the mean line
plt.text(1.25, mean_y_09_seeded + 0.15, r'$\bar{y}$', color='gray')

# Increase extent of x-axis
xax = plt.xlim(0.75, 5.25)

plt.xlabel('x')
plt.ylabel('y')

# Coordinates for the fourth data point and original line
x_4th = x[-1]
y_data_point = y_new_09_seeded[-1]
y_original_line = 1.5 * x_4th + 0.25

# Add a vertical right curly brace
cb1 = curlyBrace(
      fig=plt.gcf(),
      ax=plt.gca(),  # The current axes
      p1=[x_4th + 0.1, y_data_point],
      p2=[x_4th + 0.1, y_original_line],
      color='gray'
  )

cb2 = curlyBrace(
      fig=plt.gcf(),
      ax=plt.gca(),  # The current axes
      p1=[x_4th + 0.1, y_original_line],
      p2=[x_4th + 0.1, mean_y_09_seeded],
      k_r = 0.15,
      color='gray'
  )

cb3 = curlyBrace(
      fig=plt.gcf(),
      ax=plt.gca(),  # The current axes
      p1=[x_4th - 0.1, mean_y_09_seeded],
      p2=[x_4th - 0.1, y_data_point],
      k_r = 0.05,
      color='gray'
  )

# Label for the overall vertical line between the data point and the mean to be more to the left
plt.text(x[-1] - 0.5, (mean_y_09_seeded + y_new_09_seeded[-1]) / 2, r'$y_i - \bar{y}$', color='gray', fontsize = 12)

plt.tight_layout()
plt.show()
```

### Sums of squares
* Geometrically, we can see that $(y_i - \bar{y}) = (\widehat{y}_i - \bar{y}) + (y_i - \widehat{y}_i)$, and we can show that
$$
\sum_{i=1}^n (y_i - \bar{y})^2 = \sum_{i=1}^n (\widehat{y}_i - \bar{y})^2 + \sum_{i=1}^n (y_i - \widehat{y}_i)^2
$$

  ::: {style="font-size: 80%;"}
  - The term on the left is the **total sum of squares** ($\mbox{TSS}$)
  - The second term on the right hand side is the **residual sum of squares** ($\mbox{RSS}$), shown on [this](#least-squares) slide
  - The first term on the right is the **sum of squares due to the regression** ($\mbox{SSReg}$)
  :::

* So, we can write
$$
\mbox{TSS} = \mbox{SSReg} + \mbox{RSS}
$$

* The **total** variability can be partitioned into two sources: the variability explained by the regression line ($\mbox{SSReg}$), and the unexplained, or residual, variability ($\mbox{SSReg}$)
:::

## Interpreting the output: ANOVA {.smaller #interpreting-output-anova}

* The analysis of variance (ANOVA) table shows the partition of the sums of squares (see $\mathsf{\mbox{sum_sq}}$ in the 'ANOVA' tab of [this slide](#OLS-code-output))

  ::: {style="font-size: 80%;"}
  - $\mbox{SSReg} = `{python} fit_thick_model.ess.round(3)`$; $\mbox{RSS} = `{python} fit_thick_model.ssr.round(3)`$; $\mbox{TSS} = `{python} fit_thick_model.centered_tss.round(3)`$

  - The $F$-test assesses whether the variablity explained by the regression is 'large' compared to the unexplained variability; the $F$-statistic is the ratio of the two, taking into account the number of pieces of information (degrees of freedom, $\mathsf{\mbox{df}}$)

  - When there is only one explanatory variable, the $F$-test and $t-$test are equivalent (the value of the $F$-statistic is the square of the $t$-statistic)

  - When there are $p$ explanatory variables, $x_1, x_2, \ldots, x_p$, ANOVA can be used to test the hypothesis
$$
\beta_1 = \beta_2 = \ldots = \beta_p = 0
$$
against the alternative that at least one of them is not equal to zero.
  :::

* The RSS divided by its degrees of freedom ($`{python} fit_thick_model.mse_resid.round(3)`$) yields an estimate of the variance of the scatter, or 'error' about the fitted line.

## Interpreting $R^2$ {.smaller}
* $R^2$ is the proportion of the total variability ($\mbox{TSS}$) that can be explained, or accounted for, by the regression. It can be written as
$$
R^2 = \frac{\mbox{SSReg}}{\mbox{TSS}} = 1 - \frac{\mbox{RSS}}{\mbox{TSS}}
$$
* $R^2$ takes on values between 0 and 1

  ::: {style="font-size: 80%;"}
  - $R^2$ close to 1 indicates a large proportion of overall variability is accounted for by the regression
  - Small $R^2$ indicates that the regression does not explain much of the variability
    - Can occur if the linear model is wrong, or if the error variance $\sigma^2$ is large, or both
  :::

* **By itself**, $R^2$ can be a misleading measure of the goodness-of-fit of the linear model

  ::: {style="font-size: 80%;"}
  - Increasing the range of the explanatory variable(s) will increase $R^2$
  - Adding additional (even irrelevant) explanatory variables can increase $R^2$
  :::

## Hazards of $R^2$: Anscombe quartet

```{python}
# Load the dataset
file_path = 'data/noisyAnscombe.csv'
anscombe_data = pd.read_csv(file_path)

# Define x and y columns for each dataset
x_sets = ['x1', 'x2', 'x3', 'x4']
y_sets = ['y1', 'y2', 'y3', 'y4']
```

```{python}
# Define a function to plot the data and optionally include fitted lines
def plot_anscombe(data, x_cols, y_cols, include_fit=False):
    fig, axes = plt.subplots(1, 4, figsize=(12, 3))  # 1 row, 4 columns

    for i, (x_col, y_col) in enumerate(zip(x_cols, y_cols)):
        ax = axes[i]  # Index directly into the 1D axes array

        # Scatter plot the data
        ax.scatter(data[x_col], data[y_col], color='green', marker='o')

        if include_fit:
            # Fit a linear model using statsmodels formula API
            formula = f"{y_col} ~ {x_col}"
            model = smf.ols(formula=formula, data=data).fit()
            y_pred = model.fittedvalues

            # Plot the fitted line
            ax.plot(data[x_col], y_pred, color='red', linestyle='-', linewidth=2)

            # Extract slope and R^2 from statsmodels results
            slope = model.params[1]
            r_squared = model.rsquared

            # Annotate with LaTeX expressions for slope and R^2
            ax.text(0.05, 0.95, f"$\\widehat{{\\beta}}_1 = {slope:.2f}$\n$R^2 = {r_squared:.2f}$",
                    transform=ax.transAxes, fontsize=12, verticalalignment='top')

        # Use LaTeX for axis labels with subscripts
        ax.set_xlabel(f"${x_col[0]}_{x_col[1]}$")
        ax.set_ylabel(f"${y_col[0]}_{y_col[1]}$")

    # Use tight_layout to reduce whitespace
    plt.tight_layout()
    return fig
```


::: {.panel-tabset}

### Data

```{python}
plot_anscombe(anscombe_data, x_sets, y_sets, include_fit=False)
plt.show()
```

"Graphs are essential to good statistical analysis."

::: {style="font-size: 50%;"}
Anscombe, F.J. (1973) Graphs in Statistical Analysis, _The American Statistician_, **27** (1), 17--21.
:::

### Fitted lines

```{python}
# Plot the data with the fitted lines
plot_anscombe(anscombe_data, x_sets, y_sets, include_fit=True)
plt.show()
```

Moral: _Never use $R^2$ by itself as a measure of goodness-of-fit!_

:::

# Assessing goodness-of-fit

## Checking the conditions for inference

* The conditions for inference involve the true, but unknown, regression line and deviations from this line

* We can't observe this line, but the regression line is our best estimate, and the residuals $(y_i - \widehat{y}_i)$ estimate the deviations

* We check the conditions for regression inference by looking and evaluating graphs of standardized residuals

## Condition 1

::: {.panel-tabset}

### Condition

_Response varies with a Gaussian distribution about the regression line_

* The deviations from the population regression line---estimated by the residuals---must have (approximately) a Gaussian distribution

* Check a histogram of the residuals---is it roughly symmetric, or are there major departures from a Gaussian distribution?

### Histogram

```{python}
#| fig-align: center
plt.figure(figsize = (5, 5))
plt.hist(fit_thick_model.resid_pearson, color = 'green', density = 'True')
plt.xlabel('standardized residuals')
plt.ylabel('density')
plt.show()
```



### Q-Q Plot

```{python}
#| fig-align: center
fig, ax = plt.subplots(figsize = (5, 5))
sm.qqplot(fit_thick_model.resid_pearson, line = "45", ax = ax)
plt.show()
```

:::

## Condition 2

::: {.panel-tabset}

### Condition

_The standard deviation of the response(s) is the same for all values of the explanatory variable(s)_

* Look at a scatter plot of the standardized residual against the explanatory variable

* The scatter should be roughly the same from one end to the other

### Plot

```{python}
#| fig-align: center
plt.scatter(thick_data['time'], fit_thick_model.resid_pearson, color = 'green')
plt.xlabel('time')
plt.ylabel('standardized residuals')
plt.axhline(y = 0, color='gray', linestyle='-.')
plt.show()
```

:::

## Condition 3

::: {.panel-tabset}

### Condition

_The relationship is linear in the population_

::: {style="font-size: 80%;"}
* Look at a scatter plot of the standardized residuals against fitted values $\widehat{y}$

* Are there are any curved patterns or departures from linearity?

* Can use scatterplot of $y$ and $x$, but residual plots magnify any departures

* Pattern of points will be similar to Condition 1 when there is only one explanatory variable
:::

### Plot

```{python}
#| fig-align: center
plt.scatter(fit_thick_model.fittedvalues, fit_thick_model.resid_pearson, color = 'green')
plt.xlabel('fitted thickness')
plt.ylabel('standardized residuals')
plt.axhline(y = 0, color='gray', linestyle='-.')
plt.show()
```

:::

## Condition 4

::: {.panel-tabset}

### Condition

_The observations are independent_

* Use information about how the data were collected to determine whether observations are independent

* Do the residuals appear correlated? For example, do positive residuals tend to clump together?

### Plot

```{python}
#| fig-align: center
plt.plot(thick_data['time'], fit_thick_model.resid_pearson, color = 'green')
plt.xlabel('time')
plt.ylabel('standardized residuals')
plt.axhline(y = 0, color='gray', linestyle='-.')
plt.show()
```

:::

## Degradation model: conclusions

::: {style="font-size: 90%"}
* There does appear to be a linear relationship between thickness reduction and time

* The regression model accounts for about 85% of the total variation in the data; nevertheless, are there additional explanatory variables that might help explain additional variability, e.g., fluid characteristics?

* Although the dataset is relatively small, the residuals do not show any departures from the assumptions

* We _tentatively_ accept the model and proceed with inference and predictions 
:::

## Predicting degradation {.smaller}

::: {.columns}
::: {.column width="55%"}

* At position A1, after linear fitting we have the following model:
  * $y = 13.9169 - 0.8349 t$
* To predict pipe thicknesses in the future we simply subsitute in values for time $t$:
  * e.g. $t = 10$ the predicted thickness is $y = 13.9169 - 0.8349 * 10 = 5.6$ mm
* To determine _when_ the pipe will cross the critical thickness of 2 mm, we solve for $t$:
  * $t = \frac{y - 13.9169}{-0.8349} = \frac{2 - 13.9169}{-0.8349} = 14.3$ days

:::
::: {.column width="45%"}

```{python}
#| echo: false
thick_data = pd.read_csv("data/thickness.csv")
thick_model = smf.ols(formula="thickness ~ time", data=thick_data).fit()

# X = sm.add_constant(thick_data["time"]) # Add a column of 1s
# thick_model = sm.OLS(thick_data["thickness"], X).fit()

fig, ax = plt.subplots(figsize=(4, 4))

ax.scatter(thick_data["time"], thick_data["thickness"])
xs = np.array([0, thick_data["time"].max()])
ax.plot(xs, thick_model.params.iloc[0] + thick_model.params.iloc[1] * xs, color="red")
xs = np.array([thick_data["time"].max(), 20])
ax.plot(xs, thick_model.params.iloc[0] + thick_model.params.iloc[1] * xs, color="red", linestyle="dashed")

tcrit = (2 - thick_model.params.iloc[0]) / thick_model.params.iloc[1]
ax.hlines(2, 0, tcrit, colors="black", linestyle="dashed")
ax.vlines(tcrit, 0, 2, colors="black", linestyle="dashed")

ax.set_ylim(ymin=0, ymax=18)
ax.set_xlim(xmin=0, xmax=18)

ax.set_xlabel("Time [day]")
ax.set_ylabel("Thickness [mm]")

plt.show()
```

:::
:::

---

## Taking uncertainty into account

::: {.callout-warning}
### What about our uncertainty?
So far we have ignored the uncertainty included in our linear fit and must propagate this uncertainty into predictions.
:::

## Handling uncertainty {.smaller}

::: {.columns}
::: {.column width="40%"}

```{python}
#| echo: false
prediction = thick_model.get_prediction(thick_data).summary_frame(alpha=0.05)

fig, ax = plt.subplots(figsize=(5, 5))
ax.fill_between(thick_data["time"], prediction["obs_ci_lower"], prediction["obs_ci_upper"], color="blue", alpha=0.25, edgecolor=None)
ax.fill_between(thick_data["time"], prediction["mean_ci_lower"], prediction["mean_ci_upper"], color="red", alpha=0.25, edgecolor=None)
ax.plot(thick_data["time"], prediction["mean"], color="red")

new_times = pd.DataFrame({"time": np.linspace(8, 18, 50)})
prediction = thick_model.get_prediction(exog=new_times).summary_frame(alpha=0.05)
ax.fill_between(new_times["time"], prediction["obs_ci_lower"], prediction["obs_ci_upper"], color="blue", alpha=0.25, edgecolor=None)
ax.fill_between(new_times["time"], prediction["mean_ci_lower"], prediction["mean_ci_upper"], color="red", alpha=0.25, edgecolor=None)
ax.plot(new_times["time"], prediction["mean"], color="red", linestyle="dashed")

ax.scatter(thick_data["time"], thick_data["thickness"])

ax.hlines(2, 0, 18, color="black", linestyle="dashed")
ax.set_ylim(0, 18)
ax.set_xlim(0, 18)
plt.show()
```

:::
::: {.column width="60%"}

* Each measured $y$ value has an associated error, and this means the model fit must also includes an uncertainty term
* **<span style="color: red;">Confidence interval</span>** is the region where we believe the underlying linear relationship is most likely to be given the uncertainty in the parameter estimation
* **<span style="color: blue;">Prediction interval</span>** is the region where we believe new observations would lie, given the uncertainty in our model _and_ assuming the future measurements have the same variability

:::
:::

## Handling uncertainty 

::: {style="font-size: 80%"}
* A prediction interval is an uncertainty interval for $y$ at some value $x_0$
* The expression for the standard error of prediction (SEP) depends on:
  * The number of observations, $n$, in the data set: the larger the data set, the smaller the SEP
  * The squared difference $(x_n - \bar{x})^2$ between the future value of time and the mean value of the times in the training set. 
    * The larger the difference, i.e., the further in time that we want the make the prediction, the larger the SEP
  * The scatter, or noise, about the line: the greater the scatter, the greater the SEP
:::


## Prediction intervals in Python {.smaller}

* `statsmodels` can compute both confidence and prediction intervals for us
* Example: find the 95% confidence and prediction intervals at $t=9$ and $10$:

```{python}
#| echo: true
# Let's predict at time 9 and 11
# Create data frame with values at which we want predictions
new_times = pd.DataFrame({'time': [9, 10, 11]})

# Generate predictions and 95% prediction intervals
prediction = thick_model.get_prediction(exog = new_times)
prediction.summary_frame(alpha = 0.05)  # 95% percentile window
```

* At $t = 10$, with 95% confidence:
  * the mean value of pipe thickness lies between $4.32$ and $6.81$ mm
  * the predicted pipe thickness will lie between $3.26$ and $7.87$ mm
  
## Degradation modelling: the fine print

:::: {.columns}

::: {.column width="50%"}
::: {style="font-size: 70%"}
* In this example, we're predicting the value of thickness at given times. 
* What we'd like to have is a confidence interval for the failure time; it's possible to graphically extrapolate the intervals for thickness ('extrapolated intervals')
* In reliability analysis, it is much more common to ask the question, "What is the probability of failure (however defined) at some time $t$ and what is the interval for the _probability_?"
  * Can do so by calculating the _failure time distribution_
:::
:::

::: {.column width="50%"}
![](images/FT.png)
:::

::::